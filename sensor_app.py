import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# ì‚¬ì´í‚·ëŸ° ëª¨ë¸ ì„í¬íŠ¸
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_squared_error

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Sensor ML Expert Pro", layout="wide")

# ëª¨ë¸ ì„¤ëª… ë°ì´í„°ë² ì´ìŠ¤ (ë²¤ì¹˜ë§ˆí‚¹ìš©)
model_info = {
    "Linear Regression": {
        "desc": "ê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë¸ë¡œ, ì˜¨ë„/ë†ë„ì™€ ì €í•­ì´ ì§ì„  ê´€ê³„ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.",
        "pros": "ê³„ì‚°ì´ ë¹ ë¥´ê³  ìˆ˜ì‹($y=ax+b$)ì´ ëª…í™•í•´ ë¬¼ë¦¬ì  í•´ì„ì´ ê°€ì¥ ì‰½ìŠµë‹ˆë‹¤.",
        "cons": "ë³µì¡í•œ ë¹„ì„ í˜• ë°ì´í„°(ê³¡ì„  ë“±)ëŠ” ì˜ ë§ì¶”ì§€ ëª»í•©ë‹ˆë‹¤.",
        "best_for": "ê¸°ë³¸ ë³´ì •ì‹ ì‚°ì¶œìš©"
    },
    "Ridge Regression": {
        "desc": "ì„ í˜• íšŒê·€ì— 'ê·œì œ'ë¥¼ ë”í•´ ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•´ì§€ëŠ” ê²ƒì„ ë§‰ìŠµë‹ˆë‹¤.",
        "pros": "ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ë§ì„ ë•Œ ì„ í˜• íšŒê·€ë³´ë‹¤ ì•ˆì •ì ì…ë‹ˆë‹¤.",
        "cons": "ì—¬ì „íˆ ì§ì„ ì ì¸ ê´€ê³„ë§Œ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "best_for": "ë°ì´í„°ê°€ ì ê³  ë…¸ì´ì¦ˆê°€ ìˆì„ ë•Œ"
    },
    "Lasso Regression": {
        "desc": "ì¤‘ìš”í•˜ì§€ ì•Šì€ ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦¬ëŠ” ê·œì œ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ë³€ìˆ˜ê°€ ë§ì„ ë•Œ ì •ë§ ì¤‘ìš”í•œ ë³€ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ ê±¸ëŸ¬ë‚´ê¸° ì¢‹ìŠµë‹ˆë‹¤.",
        "cons": "ë³€ìˆ˜ê°€ ì ì€ ê²½ìš° ì¼ë°˜ ì„ í˜• ëª¨ë¸ê³¼ í° ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "best_for": "ë³€ìˆ˜ ì„ íƒ ë° ê°„ì†Œí™”"
    },
    "ElasticNet": {
        "desc": "Ridgeì™€ Lassoì˜ ì¥ì ì„ í•©ì¹œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ì—¬ëŸ¬ ë³€ìˆ˜ê°€ ì„œë¡œ ì–½í˜€ ìˆì„ ë•Œ(ë‹¤ì¤‘ê³µì„ ì„±) íš¨ê³¼ì ì…ë‹ˆë‹¤.",
        "cons": "ì„¤ì •í•´ì•¼ í•  íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ ë‹¤ë£¨ê¸° ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤.",
        "best_for": "ë³µí•©ì ì¸ í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬"
    },
    "Huber Regressor (Robust)": {
        "desc": "ì´ìƒì¹˜(íŠ€ëŠ” ê°’)ì— ë§¤ìš° ê°•í•œ ì„ í˜• ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ì„¼ì„œ ë…¸ì´ì¦ˆë‚˜ ì¼ì‹œì  íŠ€ëŠ” ê°’ì— íœ˜ë‘˜ë¦¬ì§€ ì•Šê³  ëŒ€ì„¸ ìˆ˜ì‹ì„ ì°¾ìŠµë‹ˆë‹¤.",
        "cons": "ë°ì´í„° ì „ì²´ê°€ ê¹¨ë—í•˜ë‹¤ë©´ ì¼ë°˜ ì„ í˜• ëª¨ë¸ë³´ë‹¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ì „ê¸°ì  ë…¸ì´ì¦ˆê°€ ì‹¬í•œ ë°ì´í„°"
    },
    "SVR (Support Vector)": {
        "desc": "ë°ì´í„°ë¥¼ ê³ ì°¨ì›ìœ¼ë¡œ ë³´ë‚´ ë³µì¡í•œ ê²½ê³„ì„ ì„ ì°¾ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ë¹„ì„ í˜•ì ì¸ ì„¼ì„œ ë°˜ì‘ì„ ë§¤ìš° ì •êµí•˜ê²Œ ì¡ì•„ëƒ…ë‹ˆë‹¤.",
        "cons": "ë°ì´í„° ì–‘ì´ ë„ˆë¬´ ë§ìœ¼ë©´ í•™ìŠµ ì†ë„ê°€ ê¸‰ê²©íˆ ëŠë ¤ì§‘ë‹ˆë‹¤.",
        "best_for": "ì •ë°€í•œ ë¹„ì„ í˜• ë³´ì •"
    },
    "K-Neighbors Regressor": {
        "desc": "í˜„ì¬ ì¡°ê±´ê³¼ ê°€ì¥ ë¹„ìŠ·í•œ ê³¼ê±° ë°ì´í„° nê°œë¥¼ ì°¾ì•„ í‰ê· ì„ ëƒ…ë‹ˆë‹¤.",
        "pros": "ë°ì´í„° ë¶„í¬ë¥¼ ëª°ë¼ë„ ì§ê´€ì ìœ¼ë¡œ ì˜ ë§ì¶¥ë‹ˆë‹¤.",
        "cons": "ìˆ˜ì‹ì´ ë‚˜ì˜¤ì§€ ì•Šì•„ í•˜ë“œì›¨ì–´ ì´ì‹ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "best_for": "ë‹¨ìˆœ ì˜ˆì¸¡ ë° ì„±ëŠ¥ ë¹„êµ"
    },
    "Decision Tree": {
        "desc": "ìŠ¤ë¬´ê³ ê°œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤.",
        "pros": "ë°ì´í„°ì˜ íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤.",
        "cons": "ê³¼ì í•©(Overfitting)ë˜ê¸° ì‰¬ì›Œ ìƒˆë¡œìš´ ë°ì´í„°ì— ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ë°ì´í„° ê·œì¹™ì„± íŒŒì•…"
    },
    "Random Forest": {
        "desc": "ìˆ˜ë§ì€ ê²°ì • ë‚˜ë¬´ë¥¼ ë§Œë“¤ì–´ ì§‘ë‹¨ì§€ì„±ìœ¼ë¡œ ê²°ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.",
        "pros": "ëŒ€ë¶€ë¶„ì˜ ì„¼ì„œ ë°ì´í„°ì—ì„œ ê°€ì¥ ì•ˆì •ì ì´ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.",
        "cons": "ëª¨ë¸ì˜ ìš©ëŸ‰ì´ í¬ê³  ë‚´ë¶€ ì—°ì‚° ê³¼ì •ì„ ì´í•´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
        "best_for": "ë²”ìš©ì ì¸ ê³ ì„±ëŠ¥ ë¶„ì„"
    },
    "Extra Trees": {
        "desc": "Random Forestë³´ë‹¤ ë” ë¬´ì‘ìœ„ì„±ì„ ë¶€ì—¬í•´ ì†ë„ë¥¼ ë†’ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ì´ìƒì¹˜ì— ë” ê°•í•˜ê³  Random Forestë³´ë‹¤ ê³„ì‚°ì´ ë¹ ë¦…ë‹ˆë‹¤.",
        "cons": "ë•Œë•Œë¡œ Random Forestë³´ë‹¤ ì˜¤ì°¨ê°€ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ë¹ ë¥´ê³  ê°•ê±´í•œ ì•™ìƒë¸” í•™ìŠµ"
    },
    "AdaBoost": {
        "desc": "ì•½í•œ ëª¨ë¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œì¼œ ì´ì „ì˜ ì‹¤ìˆ˜ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤.",
        "pros": "ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ì„ ëª¨ì•„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ëŒì–´ëƒ…ë‹ˆë‹¤.",
        "cons": "ë…¸ì´ì¦ˆê°€ ë„ˆë¬´ ì‹¬í•˜ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë§ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ë‹¨ê³„ì  ì˜¤ì°¨ ìˆ˜ì •"
    },
    "Gradient Boosting": {
        "desc": "í˜„ì¬ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ê°•ë ¥í•œ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.",
        "pros": "ì˜ˆì¸¡ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì€ í¸ì— ì†í•©ë‹ˆë‹¤.",
        "cons": "í•™ìŠµ ì‹œê°„ì´ ê¸¸ê³  íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.",
        "best_for": "ì •í™•ë„ ìµœìš°ì„  ì‹œ"
    }
}

# --- ì‚¬ì´ë“œë°” ë©”ë‰´ êµ¬ì„± ---
st.sidebar.title("ğŸ› ï¸ ë¶„ì„ ë©”ë‰´")
app_mode = st.sidebar.radio("ì›í•˜ëŠ” ë¶„ì„ ê¸°ëŠ¥ì„ ì„ íƒí•˜ì„¸ìš”", 
                           ["1. ì „ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹", "2. ë…¸í™” ì§„ë‹¨ ë° ë¯¸ë˜ ì˜ˆì¸¡"])

# --- 1. ë°ì´í„° ë¡œë“œ (ê³µí†µ ì„¹ì…˜) ---
st.title("ğŸ§ª ì„¼ì„œ ì •ë°€ ë¶„ì„ ì‹œìŠ¤í…œ")
uploaded_file = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì˜¨ë„, ìŠµë„, ì €í•­ í•„ìˆ˜)", type="csv")

if uploaded_file is not None:
    # ë°ì´í„° ì½ê¸° ë° ì»¬ëŸ¼ ì •ë¦¬
    df = pd.read_csv(uploaded_file)
    df.columns = [col.strip() for col in df.columns]
    
    # ê³µí†µ ë¬¼ë¦¬ ë³€í™˜ ë° ì‹œê°„ ì²˜ë¦¬
    df['Resistance_kOhm'] = df['ì €í•­'] / 1000.0
    
    if 'ì¸¡ì • ì‹œê°„' in df.columns:
        df['ì¸¡ì • ì‹œê°„'] = pd.to_datetime(df['ì¸¡ì • ì‹œê°„'])
        first_time = df['ì¸¡ì • ì‹œê°„'].min()
        df['Elapsed_Days'] = (df['ì¸¡ì • ì‹œê°„'] - first_time).dt.total_seconds() / (24 * 3600)
    else:
        df['Elapsed_Days'] = np.arange(len(df)) / (60 * 24)

    # ìŠµë„ ë¬¼ë¦¬ ë³€í™˜ (Humidity_ppm ê³„ì‚°)
    p_sat = 6.112 * np.exp((17.62 * df['ì˜¨ë„']) / (243.12 + df['ì˜¨ë„']))
    df['Humidity_ppm'] = ((df['ìŠµë„'] / 100) * p_sat / 1013.25) * 1_000_000
    df['Temp_K'] = df['ì˜¨ë„'] + 273.15

    # ---------------------------------------------------------
    # MODE 1: ì „ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ (ì²« ë²ˆì§¸ ì½”ë“œ ë¡œì§)
    # ---------------------------------------------------------
    if app_mode == "1. ì „ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹":
        st.sidebar.divider()
        st.sidebar.header("ğŸ¤– ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì„¤ì •")
        selected_model_name = st.sidebar.selectbox("í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", list(model_info.keys()))

        with st.sidebar.expander("ğŸ’¡ ì„ íƒëœ ëª¨ë¸ íŠ¹ì„± ë³´ê¸°", expanded=True):
            info = model_info[selected_model_name]
            st.markdown(f"**í•œì¤„í‰:** {info['desc']}")
            st.markdown(f"âœ… **ì¥ì :** {info['pros']}")
            st.markdown(f"âŒ **ë‹¨ì :** {info['cons']}")
            st.success(f"ğŸ¯ **ì¶”ì²œ:** {info['best_for']}")

        model_dict = {
            "Linear Regression": LinearRegression(),
            "Ridge Regression": Ridge(alpha=1.0),
            "Lasso Regression": Lasso(alpha=0.1),
            "ElasticNet": ElasticNet(alpha=0.1),
            "Huber Regressor (Robust)": HuberRegressor(),
            "SVR (Support Vector)": SVR(kernel='rbf', C=100, gamma=0.1),
            "K-Neighbors Regressor": KNeighborsRegressor(n_neighbors=5),
            "Decision Tree": DecisionTreeRegressor(max_depth=10),
            "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
            "Extra Trees": ExtraTreesRegressor(n_estimators=100, random_state=42),
            "AdaBoost": AdaBoostRegressor(n_estimators=100, random_state=42),
            "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42)
        }
        
        model = model_dict[selected_model_name]
        X = df[['Temp_K', 'Humidity_ppm']]
        y = df['Resistance_kOhm']
        
        with st.spinner(f'{selected_model_name} í•™ìŠµ ì¤‘...'):
            model.fit(X, y)
            y_pred = model.predict(X)
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mean_squared_error(y, y_pred))

        # ë¦¬í¬íŠ¸ ì„¹ì…˜
        st.divider()
        col_rep1, col_rep2 = st.columns([1.5, 1])
        with col_rep1:
            st.subheader(f"ğŸ“ {selected_model_name} ë¶„ì„ ê²°ê³¼")
            if hasattr(model, 'coef_'):
                coef = model.coef_.flatten()
                intercept = model.intercept_
                st.info(f"**ê³µì‹:** $R(k\Omega) = {intercept:.2f} + ({coef[0]:.4f} \\times T_K) + ({coef[1]:.6f} \\times H_{{ppm}})$")
            elif hasattr(model, 'feature_importances_'):
                feat_imp = pd.Series(model.feature_importances_, index=['Temp(K)', 'Humidity(ppm)'])
                fig_imp, ax_imp = plt.subplots(figsize=(5, 2))
                feat_imp.sort_values().plot(kind='barh', color=['#3498db', '#e74c3c'], ax=ax_imp)
                ax_imp.set_title("Feature Importance", fontsize=10)
                st.pyplot(fig_imp)
            else:
                st.warning("ì´ ëª¨ë¸ì€ ëª…ì‹œì ì¸ ìˆ˜ì‹ì´ë‚˜ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        with col_rep2:
            st.subheader("ğŸ¯ ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥")
            st.metric("ê²°ì •ê³„ìˆ˜ ($R^2$)", f"{r2:.4f}")
            st.metric("í‰ê·  ì˜¤ì°¨ (RMSE)", f"{rmse:.4f} kÎ©")

        # ì‹œê°í™”
        st.divider()
        st.header("ğŸ“ˆ ì˜ˆì¸¡ ì„±ëŠ¥ ì‹œê°í™”")
        plt.rcdefaults()
        sns.set_theme(style="whitegrid")
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        axes[0].scatter(y, y_pred, alpha=0.2, s=2, color='darkblue')
        axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
        axes[0].set_title(f"Measured vs Predicted ($R^2$={r2:.4f})")
        axes[0].set_xlabel("Measured (kOhm)")
        axes[0].set_ylabel("Predicted (kOhm)")
        residuals = y - y_pred
        sns.histplot(residuals, kde=True, ax=axes[1], color='purple')
        axes[1].set_title("Residuals Distribution (Error)")
        st.pyplot(fig)

        # ì „ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìˆœìœ„
        if st.sidebar.button("ğŸ† ì „ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ ë³´ê¸°"):
            st.divider()
            st.header("ğŸ† ì „ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìˆœìœ„")
            results = []
            with st.spinner('ì „ì²´ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘...'):
                for name, m in model_dict.items():
                    m.fit(X, y)
                    p = m.predict(X)
                    results.append({
                        "Model": name,
                        "RÂ² Score": r2_score(y, p),
                        "RMSE (kÎ©)": np.sqrt(mean_squared_error(y, p)),
                        "Best For": model_info[name]['best_for']
                    })
            res_df = pd.DataFrame(results).sort_values(by="RÂ² Score", ascending=False)
            st.table(res_df)

    # ---------------------------------------------------------
    # MODE 2: ë…¸í™” ì§„ë‹¨ ë° ë¯¸ë˜ ì˜ˆì¸¡ (ë‘ ë²ˆì§¸ ì½”ë“œ ë¡œì§)
    # ---------------------------------------------------------
    elif app_mode == "2. ë…¸í™” ì§„ë‹¨ ë° ë¯¸ë˜ ì˜ˆì¸¡":
        st.sidebar.divider()
        st.sidebar.header("ğŸ¤– ë…¸í™” ì˜ˆì¸¡ ëª¨ë¸ ì„¤ì •")
        model_choice = st.sidebar.selectbox(
            "ì ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
            [
                "1. Linear Regression (ì„ í˜•)", 
                "2. Ridge Regression (ê·œì œ ì„ í˜•)", 
                "3. Decision Tree (ì˜ì‚¬ê²°ì • ë‚˜ë¬´)", 
                "4. Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)", 
                "5. Gradient Boosting (ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…)"
            ]
        )
        st.sidebar.warning("âš ï¸ ë¯¸ë˜ ì˜ˆì¸¡(ë‚ ì§œ ë³€ê²½)ì€ 1, 2ë²ˆ ì„ í˜• ëª¨ë¸ì—ì„œë§Œ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")

        # í•™ìŠµ ë³€ìˆ˜ ì •ì˜ (ë…¸í™” ë¶„ì„ì„ ìœ„í•´ Elapsed_Days í¬í•¨)
        X_cols = ['ì˜¨ë„', 'ìŠµë„', 'Elapsed_Days']
        X = df[X_cols]
        y = df['Resistance_kOhm']
        
        # ëª¨ë¸ ê°ì²´ ìƒì„±
        if "1." in model_choice: model = LinearRegression()
        elif "2." in model_choice: model = Ridge(alpha=1.0)
        elif "3." in model_choice: model = DecisionTreeRegressor(max_depth=10)
        elif "4." in model_choice: model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)
        elif "5." in model_choice: model = GradientBoostingRegressor(n_estimators=50, random_state=42)

        with st.spinner(f'{model_choice} ë¶„ì„ ì¤‘...'):
            model.fit(X, y)
            y_pred = model.predict(X)
            r2 = r2_score(y, y_pred)
            rmse = np.sqrt(mean_squared_error(y, y_pred))

        # ë¶„ì„ ë¦¬í¬íŠ¸
        st.divider()
        col_rep1, col_rep2 = st.columns([1.5, 1])
        
        with col_rep1:
            st.subheader("ğŸ“Š ì„¼ì„œ ìƒíƒœ ë° ì—´í™” ì§„ë‹¨")
            aging_analyzer = LinearRegression().fit(X, y)
            degradation_rate = aging_analyzer.coef_[2] 
            
            if degradation_rate > 0:
                st.warning(f"âš ï¸ **í˜„ì¬ ìƒíƒœ: ì—´í™” ì§„í–‰ ì¤‘ (ì €í•­ ì¦ê°€)**")
                st.write(f"ì˜¨ìŠµë„ ê³ ì • ì‹œ, í•˜ë£¨ í‰ê·  **{degradation_rate:.4f} kÎ©**ì”© ìƒìŠ¹ ì¤‘ì…ë‹ˆë‹¤.")
            else:
                st.success(f"âœ… **í˜„ì¬ ìƒíƒœ: ì•ˆì •í™”/í™œì„±í™” ì¤‘ (ì €í•­ ê°ì†Œ)**")
                st.write(f"ì˜¨ìŠµë„ ê³ ì • ì‹œ, í•˜ë£¨ í‰ê·  **{abs(degradation_rate):.4f} kÎ©**ì”© í•˜ê°• ì¤‘ì…ë‹ˆë‹¤.")
                
            if hasattr(model, 'coef_'):
                st.info(f"**Regression Formula:** $R = {model.intercept_:.2f} + ({model.coef_[0]:.4f} \\cdot T) + ({model.coef_[1]:.4f} \\cdot H) + ({model.coef_[2]:.4f} \\cdot Day)$")
            elif hasattr(model, 'feature_importances_'):
                plt.rcdefaults()
                fig_imp, ax_imp = plt.subplots(figsize=(5, 2.2))
                feat_imp = pd.Series(model.feature_importances_, index=['Temp', 'Humi', 'Aging'])
                feat_imp.sort_values().plot(kind='barh', color='#3498db', ax=ax_imp)
                ax_imp.set_title("Feature Importance (Relative Impact)", fontsize=10)
                st.pyplot(fig_imp)

        with col_rep2:
            st.subheader("ğŸ¯ ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥")
            st.metric("ê²°ì •ê³„ìˆ˜ (RÂ²)", f"{r2:.4f}")
            st.metric("í‰ê·  ì˜¤ì°¨ (RMSE)", f"{rmse:.4f} kÎ©")

        # ë¯¸ë˜ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´í„°
        st.divider()
        st.header("ğŸ”® ë¯¸ë˜ ì €í•­ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´í„°")
        s_col1, s_col2, s_col3, s_res = st.columns([1, 1, 1, 2])
        with s_col1:
            s_temp = st.number_input("ì˜ˆìƒ ì˜¨ë„ (Â°C)", value=float(df['ì˜¨ë„'].mean()))
        with s_col2:
            s_humi = st.number_input("ì˜ˆìƒ ìŠµë„ (%)", value=float(df['ìŠµë„'].mean()))
        with s_col3:
            s_days = st.number_input("ì¶”ê°€ ì‚¬ìš©ì¼ (ì˜¤ëŠ˜+Nì¼)", value=30, step=1)
        
        target_day = df['Elapsed_Days'].max() + s_days
        input_data = pd.DataFrame([[s_temp, s_humi, target_day]], columns=['ì˜¨ë„', 'ìŠµë„', 'Elapsed_Days'])
        future_val = model.predict(input_data)[0]
        
        with s_res:
            st.metric(f"{s_days}ì¼ í›„ ì˜ˆìƒ ì €í•­", f"{future_val:.4f} kÎ©")
            diff = future_val - df['Resistance_kOhm'].iloc[-1]
            st.write(f"í˜„ì¬ ë§ˆì§€ë§‰ ì¸¡ì •ê°’ ëŒ€ë¹„ ë³€í™”ëŸ‰: **{diff:+.4f} kÎ©**")

        # ì‹œê°í™” ì„¹ì…˜ (4ë‹¨)
        st.divider()
        st.header("ğŸ“ˆ ì˜í–¥ë„ ë° ì„±ëŠ¥ ìƒì„¸ ë¶„ì„")
        plt.rcdefaults()
        sns.set_theme(style="whitegrid")
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        sns.regplot(ax=axes[0, 0], x='ì˜¨ë„', y='Resistance_kOhm', data=df, 
                    scatter_kws={'alpha': 0.02, 's': 1, 'color': 'gray'}, line_kws={'color': 'red'})
        axes[0, 0].set_title("Temperature vs Resistance", fontsize=12)

        temp_humi_effect = aging_analyzer.coef_[0] * df['ì˜¨ë„'] + aging_analyzer.coef_[1] * df['ìŠµë„'] + aging_analyzer.intercept_
        drift_only = df['Resistance_kOhm'] - temp_humi_effect
        axes[0, 1].scatter(df['Elapsed_Days'], drift_only, alpha=0.05, s=1, color='orange')
        axes[0, 1].set_title("Pure Aging Drift (T/H Removed)", fontsize=12)

        axes[1, 0].scatter(y, y_pred, alpha=0.1, s=1, color='purple')
        axes[1, 0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=1.5)
        axes[1, 0].set_title(f"Model Linearity (R2={r2:.4f})", fontsize=12)

        sample_df = df.iloc[::30]
        axes[1, 1].plot(sample_df['ì¸¡ì • ì‹œê°„'], sample_df['Resistance_kOhm'], label='Measured', alpha=0.5, color='black', lw=1)
        axes[1, 1].plot(sample_df['ì¸¡ì • ì‹œê°„'], y_pred[::30], label='ML Predicted', color='limegreen', linestyle='--', lw=1.5)
        axes[1, 1].set_title("Real-time Tracking Performance", fontsize=12)
        axes[1, 1].legend(prop={'size': 8})
        plt.tight_layout()
        st.pyplot(fig)

    # 8. ê³µí†µ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    st.divider()
    st.download_button("ìµœì¢… ë¶„ì„ ë°ì´í„° ë°›ê¸°", df.to_csv(index=False).encode('utf-8'), "sensor_analysis_result.csv")

else:
    st.info("ğŸ‘‹ ì„¼ì„œ ë°ì´í„° CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš” (ì˜¨ë„, ìŠµë„, ì €í•­ ì»¬ëŸ¼ í¬í•¨).")