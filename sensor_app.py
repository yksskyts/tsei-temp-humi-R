import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ì‚¬ì´í‚·ëŸ° ëª¨ë¸ ì„í¬íŠ¸
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_squared_error

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Sensor ML Benchmarking", layout="wide")

# ëª¨ë¸ ì„¤ëª… ë°ì´í„°ë² ì´ìŠ¤
model_info = {
    "Linear Regression": {
        "desc": "ê°€ì¥ ê¸°ë³¸ì ì¸ ëª¨ë¸ë¡œ, ì˜¨ë„/ë†ë„ì™€ ì €í•­ì´ ì§ì„  ê´€ê³„ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.",
        "pros": "ê³„ì‚°ì´ ë¹ ë¥´ê³  ìˆ˜ì‹($y=ax+b$)ì´ ëª…í™•í•´ ë¬¼ë¦¬ì  í•´ì„ì´ ê°€ì¥ ì‰½ìŠµë‹ˆë‹¤.",
        "cons": "ë³µì¡í•œ ë¹„ì„ í˜• ë°ì´í„°(ê³¡ì„  ë“±)ëŠ” ì˜ ë§ì¶”ì§€ ëª»í•©ë‹ˆë‹¤.",
        "best_for": "ê¸°ë³¸ ë³´ì •ì‹ ì‚°ì¶œìš©"
    },
    "Ridge Regression": {
        "desc": "ì„ í˜• íšŒê·€ì— 'ê·œì œ'ë¥¼ ë”í•´ ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•´ì§€ëŠ” ê²ƒì„ ë§‰ìŠµë‹ˆë‹¤.",
        "pros": "ë°ì´í„°ì— ë…¸ì´ì¦ˆê°€ ë§ì„ ë•Œ ì„ í˜• íšŒê·€ë³´ë‹¤ ì•ˆì •ì ì…ë‹ˆë‹¤.",
        "cons": "ì—¬ì „íˆ ì§ì„ ì ì¸ ê´€ê³„ë§Œ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "best_for": "ë°ì´í„°ê°€ ì ê³  ë…¸ì´ì¦ˆê°€ ìˆì„ ë•Œ"
    },
    "Lasso Regression": {
        "desc": "ì¤‘ìš”í•˜ì§€ ì•Šì€ ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦¬ëŠ” ê·œì œ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ë³€ìˆ˜ê°€ ë§ì„ ë•Œ ì •ë§ ì¤‘ìš”í•œ ë³€ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ ê±¸ëŸ¬ë‚´ê¸° ì¢‹ìŠµë‹ˆë‹¤.",
        "cons": "ë³€ìˆ˜ê°€ ì ì€ ê²½ìš° ì¼ë°˜ ì„ í˜• ëª¨ë¸ê³¼ í° ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤.",
        "best_for": "ë³€ìˆ˜ ì„ íƒ ë° ê°„ì†Œí™”"
    },
    "ElasticNet": {
        "desc": "Ridgeì™€ Lassoì˜ ì¥ì ì„ í•©ì¹œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ì—¬ëŸ¬ ë³€ìˆ˜ê°€ ì„œë¡œ ì–½í˜€ ìˆì„ ë•Œ(ë‹¤ì¤‘ê³µì„ ì„±) íš¨ê³¼ì ì…ë‹ˆë‹¤.",
        "cons": "ì„¤ì •í•´ì•¼ í•  íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ ë‹¤ë£¨ê¸° ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤.",
        "best_for": "ë³µí•©ì ì¸ í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬"
    },
    "Huber Regressor (Robust)": {
        "desc": "ì´ìƒì¹˜(íŠ€ëŠ” ê°’)ì— ë§¤ìš° ê°•í•œ ì„ í˜• ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ì„¼ì„œ ë…¸ì´ì¦ˆë‚˜ ì¼ì‹œì  íŠ€ëŠ” ê°’ì— íœ˜ë‘˜ë¦¬ì§€ ì•Šê³  ëŒ€ì„¸ ìˆ˜ì‹ì„ ì°¾ìŠµë‹ˆë‹¤.",
        "cons": "ë°ì´í„° ì „ì²´ê°€ ê¹¨ë—í•˜ë‹¤ë©´ ì¼ë°˜ ì„ í˜• ëª¨ë¸ë³´ë‹¤ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ì „ê¸°ì  ë…¸ì´ì¦ˆê°€ ì‹¬í•œ ë°ì´í„°"
    },
    "SVR (Support Vector)": {
        "desc": "ë°ì´í„°ë¥¼ ê³ ì°¨ì›ìœ¼ë¡œ ë³´ë‚´ ë³µì¡í•œ ê²½ê³„ì„ ì„ ì°¾ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ë¹„ì„ í˜•ì ì¸ ì„¼ì„œ ë°˜ì‘ì„ ë§¤ìš° ì •êµí•˜ê²Œ ì¡ì•„ëƒ…ë‹ˆë‹¤.",
        "cons": "ë°ì´í„° ì–‘ì´ ë„ˆë¬´ ë§ìœ¼ë©´ í•™ìŠµ ì†ë„ê°€ ê¸‰ê²©íˆ ëŠë ¤ì§‘ë‹ˆë‹¤.",
        "best_for": "ì •ë°€í•œ ë¹„ì„ í˜• ë³´ì •"
    },
    "K-Neighbors Regressor": {
        "desc": "í˜„ì¬ ì¡°ê±´ê³¼ ê°€ì¥ ë¹„ìŠ·í•œ ê³¼ê±° ë°ì´í„° nê°œë¥¼ ì°¾ì•„ í‰ê· ì„ ëƒ…ë‹ˆë‹¤.",
        "pros": "ë°ì´í„° ë¶„í¬ë¥¼ ëª°ë¼ë„ ì§ê´€ì ìœ¼ë¡œ ì˜ ë§ì¶¥ë‹ˆë‹¤.",
        "cons": "ìˆ˜ì‹ì´ ë‚˜ì˜¤ì§€ ì•Šì•„ í•˜ë“œì›¨ì–´ ì´ì‹ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "best_for": "ë‹¨ìˆœ ì˜ˆì¸¡ ë° ì„±ëŠ¥ ë¹„êµ"
    },
    "Decision Tree": {
        "desc": "ìŠ¤ë¬´ê³ ê°œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤.",
        "pros": "ë°ì´í„°ì˜ íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤.",
        "cons": "ê³¼ì í•©(Overfitting)ë˜ê¸° ì‰¬ì›Œ ìƒˆë¡œìš´ ë°ì´í„°ì— ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ë°ì´í„° ê·œì¹™ì„± íŒŒì•…"
    },
    "Random Forest": {
        "desc": "ìˆ˜ë§ì€ ê²°ì • ë‚˜ë¬´ë¥¼ ë§Œë“¤ì–´ ì§‘ë‹¨ì§€ì„±ìœ¼ë¡œ ê²°ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.",
        "pros": "ëŒ€ë¶€ë¶„ì˜ ì„¼ì„œ ë°ì´í„°ì—ì„œ ê°€ì¥ ì•ˆì •ì ì´ê³  ë†’ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.",
        "cons": "ëª¨ë¸ì˜ ìš©ëŸ‰ì´ í¬ê³  ë‚´ë¶€ ì—°ì‚° ê³¼ì •ì„ ì´í•´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.",
        "best_for": "ë²”ìš©ì ì¸ ê³ ì„±ëŠ¥ ë¶„ì„"
    },
    "Extra Trees": {
        "desc": "Random Forestë³´ë‹¤ ë” ë¬´ì‘ìœ„ì„±ì„ ë¶€ì—¬í•´ ì†ë„ë¥¼ ë†’ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.",
        "pros": "ì´ìƒì¹˜ì— ë” ê°•í•˜ê³  Random Forestë³´ë‹¤ ê³„ì‚°ì´ ë¹ ë¦…ë‹ˆë‹¤.",
        "cons": "ë•Œë•Œë¡œ Random Forestë³´ë‹¤ ì˜¤ì°¨ê°€ í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ë¹ ë¥´ê³  ê°•ê±´í•œ ì•™ìƒë¸” í•™ìŠµ"
    },
    "AdaBoost": {
        "desc": "ì•½í•œ ëª¨ë¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œì¼œ ì´ì „ì˜ ì‹¤ìˆ˜ë¥¼ ë³´ì™„í•©ë‹ˆë‹¤.",
        "pros": "ë‹¨ìˆœí•œ ëª¨ë¸ë“¤ì„ ëª¨ì•„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ëŒì–´ëƒ…ë‹ˆë‹¤.",
        "cons": "ë…¸ì´ì¦ˆê°€ ë„ˆë¬´ ì‹¬í•˜ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë§ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "best_for": "ë‹¨ê³„ì  ì˜¤ì°¨ ìˆ˜ì •"
    },
    "Gradient Boosting": {
        "desc": "í˜„ì¬ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ê°•ë ¥í•œ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.",
        "pros": "ì˜ˆì¸¡ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì€ í¸ì— ì†í•©ë‹ˆë‹¤.",
        "cons": "í•™ìŠµ ì‹œê°„ì´ ê¸¸ê³  íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.",
        "best_for": "ì •í™•ë„ ìµœìš°ì„  ì‹œ"
    }
}

st.title("ğŸ§ª ì‚¬ì´í‚·ëŸ° ì „ ëª¨ë¸ ë¹„êµ: ì„¼ì„œ ë¬¼ë¦¬ ëª¨ë¸ë§")

# 2. ì‚¬ì´ë“œë°” ëª¨ë¸ ì„ íƒ
st.sidebar.header("ğŸ¤– ML ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹")
selected_model_name = st.sidebar.selectbox("í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", list(model_info.keys()))

# --- ì„ íƒëœ ëª¨ë¸ íŠ¹ì§• í‘œì‹œ (ì „ë¬¸ê°€ë‹˜ ìš”ì²­ ì‚¬í•­) ---
with st.sidebar.expander("ğŸ’¡ ì„ íƒëœ ëª¨ë¸ íŠ¹ì„± ë³´ê¸°", expanded=True):
    info = model_info[selected_model_name]
    st.markdown(f"**í•œì¤„í‰:** {info['desc']}")
    st.markdown(f"âœ… **ì¥ì :** {info['pros']}")
    st.markdown(f"âŒ **ë‹¨ì :** {info['cons']}")
    st.success(f"ğŸ¯ **ì¶”ì²œ:** {info['best_for']}")

# ëª¨ë¸ ê°ì²´ ìƒì„±
model_dict = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.1),
    "ElasticNet": ElasticNet(alpha=0.1),
    "Huber Regressor (Robust)": HuberRegressor(),
    "SVR (Support Vector)": SVR(kernel='rbf', C=100, gamma=0.1),
    "K-Neighbors Regressor": KNeighborsRegressor(n_neighbors=5),
    "Decision Tree": DecisionTreeRegressor(max_depth=10),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Extra Trees": ExtraTreesRegressor(n_estimators=100, random_state=42),
    "AdaBoost": AdaBoostRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42)
}
model = model_dict[selected_model_name]

# (ì´í›„ 3ë²ˆ~7ë²ˆ ë°ì´í„° ë¡œë“œ ë° ì‹œê°í™” ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼)
uploaded_file = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="csv")

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    df.columns = [col.strip() for col in df.columns]
    
    # [ë¬¼ë¦¬ ë³€í™˜]
    df['Resistance_kOhm'] = df['ì €í•­'] / 1000.0
    df['Temp_K'] = df['ì˜¨ë„'] + 273.15
    p_sat = 6.112 * np.exp((17.62 * df['ì˜¨ë„']) / (243.12 + df['ì˜¨ë„']))
    df['Humidity_ppm'] = ((df['ìŠµë„'] / 100) * p_sat / 1013.25) * 1_000_000
    
    X = df[['Temp_K', 'Humidity_ppm']]
    y = df['Resistance_kOhm']
    
    with st.spinner(f'{selected_model_name} í•™ìŠµ ì¤‘...'):
        model.fit(X, y)
        y_pred = model.predict(X)
        r2 = r2_score(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))

    # 5. ë¦¬í¬íŠ¸ ì„¹ì…˜
    st.divider()
    col_rep1, col_rep2 = st.columns([1.5, 1])
    with col_rep1:
        st.subheader(f"ğŸ“ {selected_model_name} ë¶„ì„ ê²°ê³¼")
        if hasattr(model, 'coef_'):
            coef = model.coef_.flatten()
            intercept = model.intercept_
            st.info(f"**ê³µì‹:** $R(k\Omega) = {intercept:.2f} + ({coef[0]:.4f} \\times T_K) + ({coef[1]:.6f} \\times H_{{ppm}})$")
        elif hasattr(model, 'feature_importances_'):
            feat_imp = pd.Series(model.feature_importances_, index=['Temp(K)', 'Humidity(ppm)'])
            fig_imp, ax_imp = plt.subplots(figsize=(5, 2))
            feat_imp.sort_values().plot(kind='barh', color=['#3498db', '#e74c3c'], ax=ax_imp)
            ax_imp.set_title("Feature Importance", fontsize=10)
            st.pyplot(fig_imp)
        else:
            st.warning("ì´ ëª¨ë¸ì€ ëª…ì‹œì ì¸ ìˆ˜ì‹ì´ë‚˜ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    with col_rep2:
        st.subheader("ğŸ¯ ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥")
        st.metric("ê²°ì •ê³„ìˆ˜ ($R^2$)", f"{r2:.4f}")
        st.metric("í‰ê·  ì˜¤ì°¨ (RMSE)", f"{rmse:.4f} kÎ©")

    # 6. ì‹œê°í™”
    st.divider()
    st.header("ğŸ“ˆ ì˜ˆì¸¡ ì„±ëŠ¥ ì‹œê°í™”")
    plt.rcdefaults()
    sns.set_theme(style="whitegrid")
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    axes[0].scatter(y, y_pred, alpha=0.2, s=2, color='darkblue')
    axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
    axes[0].set_title(f"Measured vs Predicted ($R^2$={r2:.4f})")
    axes[0].set_xlabel("Measured (kOhm)")
    axes[0].set_ylabel("Predicted (kOhm)")
    residuals = y - y_pred
    sns.histplot(residuals, kde=True, ax=axes[1], color='purple')
    axes[1].set_title("Residuals Distribution (Error)")
    st.pyplot(fig)

    # 7. ğŸ† ì „ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìˆœìœ„ (ì•ˆë‚´ ë©˜íŠ¸ ì¶”ê°€)
    if st.sidebar.button("ğŸ† ì „ ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ ë³´ê¸°"):
        st.divider()
        st.header("ğŸ† ì „ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìˆœìœ„")
        results = []
        with st.spinner('ì „ì²´ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘...'):
            for name, m in model_dict.items():
                m.fit(X, y)
                p = m.predict(X)
                results.append({
                    "Model": name,
                    "RÂ² Score": r2_score(y, p),
                    "RMSE (kÎ©)": np.sqrt(mean_squared_error(y, p)),
                    "Best For": model_info[name]['best_for'] # ìˆœìœ„í‘œì—ë„ íŠ¹ì§• ì¶”ê°€
                })
        res_df = pd.DataFrame(results).sort_values(by="RÂ² Score", ascending=False)
        st.table(res_df)
else:
    st.info("ğŸ‘‹ ì„¼ì„œ ë°ì´í„° CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë²¤ì¹˜ë§ˆí‚¹ì„ ì‹œì‘í•˜ì„¸ìš”.")